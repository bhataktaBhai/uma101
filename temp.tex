['lecture01.tex', 'lecture02.tex', 'lecture04.tex', 'lecture05.tex', 'lecture06.tex', 'lecture07.tex', 'lecture08.tex', 'lecture10.tex', 'lecture14.tex', 'lecture15.tex', 'lecture16.tex', 'lecture17.tex', 'lecture18.tex', 'lecture19.tex']
\documentclass[12pt]{article}
\input{preamble}

\title{UM101: Analysis \& Linear Algebra}
\author{Naman Mishra}
\date{Fall 2022}

\begin{document}
\maketitle
\tableofcontents

\section{Set theory \& the real number system}

\begin{defn} \label{defn:peano set}
    The set $A$ along with a successor function $S$ is called a Peano set if it obeys the Peano axioms.
    \begin{enumerate}[label=(P\arabic*)]
        \item \label{peano:zero}
            There is an element called 0 in $A$.
        \item \label{peano:succ}
            For every $a \in A$, its successor $S(a)$ is also in $A$.
        \item \label{peano:not succ}
            $\forall \; a \in A, S(a) \neq 0$.
        \item \label{peano:injective}
            For any $m, n \in A$, $S(m) = S(n)$ only if $m = n$.
        \item \label{peano:induction}
            (principle of mathematical induction) For any set $B \subseteq A$, if $0 \in B$ and $a \in B \implies S(a) \in B$, then $B = A$.
    \end{enumerate}
\end{defn}

\subsection{The ZFC Axioms}

% comment
\begin{defn} \label{defn:set}
    A \textbf{set} is a well-defined collection of (mathematical) objects, called the \emph{elements} of that set. To say that $a$ is an element of set $A$, we write $a \in A$. Otherwise, we write $a \notin A$. \\
    Given two sets $A$ and $B$, we say that:
    \begin{enumerate}
        \item[($A \subseteq B$)] $A$ is a subset of $B$, \textit{i.e.}, every element of $A$ is an element of $B$.
        \item[($A \not\subseteq B$)] $A$ is not a subset of $B$, \textit{i.e.}, there is some element in $A$ which is not an element of $B$.
        \item[($A \subsetneq B$)] $A$ is a proper subset of $B$, \textit{i.e.}, $A \subseteq B$ but $\exists\; b \in B$ such that $b \notin A$.
    \end{enumerate}
\end{defn}

\begin{rem}
    We need ZFC axioms because not any collection can be called a set. Read up on Russell's paradox.
\end{rem}

\begin{axiom}[the basic axiom] \label{zfc:basic}
    Every object is a set.
\end{axiom}

\begin{axiom}[axiom of extension] \label{zfc:extension}
    Two sets $A, B$ are equal if they have exactly the same elements. In other words, $A = B \iff A \subseteq B$ and $B \subseteq A$
\end{axiom}
\begin{rem}
    As a consequnce, it doesn't matter whether a set contains multiple copies of an element.
    \begin{align*}
        A &= \set{ 1 } \\
        B &= \set{ 1, 1, 1 }
    \end{align*}
    Clearly $A \subseteq B$ and $B \subseteq A$, implying $A = B$.
\end{rem}

\begin{axiom}[axiom of existence] \label{zfc:existence}
    There is a set with no elements called the empty set, denoted by the symbol $\varnothing$.
\end{axiom}

\begin{axiom}[axiom of specification] \label{zfc:specification}
    Let $A$ be a set. Let $P(a)$ denote a property that applies to every element in $A$, i.e., for each $a \in A$, either $P(a)$ is true or it is false. Then there exists a subset
    \[
        B = \set{a \in A: P(a) \text{ is true}}
    \]
\end{axiom}
\begin{rem}
    We are forced to create sets only as subsets of other sets because of Russell's paradox. \textcolor{red!85!black}{\emph{From MathGarden:} A somewhat surprising result is that the axiom of specification implies for each set $A$ the existence of an element (a set) $x$ such that $x \not\in A$. In other words, there is no set containing all sets of our mathematical universe.}
\end{rem}

\begin{axiom}[axiom of pairing]\label{zfc:pairing}
    Given two sets $A, B$, there exists a set which contains precisely $A, B$ as its elements, which we denote by \set{A, B}.
\end{axiom}
\begin{rem}
    In particular, by letting $A = B$, we get a set containing only $A$, i.e., \set{A}. For example, we can have $\set{\varnothing}$, and $\set{\varnothing, \{\varnothing}\}$, etc.
\end{rem}

\begin{axiom}[axiom of unions] \label{zfc:unions}
    Given a set $\mathscr{F}$ of sets, there exists a set called the union of the sets in $\mathscr{F}$, denoted by $\bigcup_{A \in \mathscr{F}} A$, whose elements are precisely the elements of the elements of $\mathscr{F}$.
    \[
        a \in \bigcup_{A \in \mathscr{F}} A \iff a \in A \text{ for some } A \in \mathscr{F}
    \]
\end{axiom}

\begin{rem}
    Intersection of a nonempty set of two or more sets and difference between two sets need not be defined as they follow from the previous axioms. (Exercise)
\end{rem}

\begin{proof}
    By the \nameref{zfc:specification},
    \begin{align*}
        A - B &= \set{ a \in A : a \not\in B } \\
        A \cap B &= \set{ a \in A : a \in B } \qedhere
    \end{align*}
\end{proof}

\begin{axiom}[axiom of powers] \label{zfc:powers}
    Given a set $A$, there exists a set called power set of $A$ denoted $\mathscr{P}(A)$, whose elements are precisely all the subsets of $A$.   
\end{axiom}
\begin{rem}
    This axiom allows us to define ordered pairs as sets (assignment) (\textcolor{red!85!black}{Isn't pairing sufficient?}) and thus direct products, relations and functions. \[
        A \times B = \set{(a, b) : a \in A, b \in B}
    \] \textcolor{red!85!black}{How does this set exist?}
\end{rem}

\textcolor{red!85!black}{How are we able to define $A \times B$, more specifically, ordered pairs?} \\
\quad \textcolor{green!30!black}{This is problem 3 of assignment 1.} 

\begin{defn} \label{defn:relation}
    A relation from set $A$ to set $B$ is a subset $R$ of $A \times B$. For any $a \in A, b \in B$ we say $a\mathrel{R}b$ iff $(a, b) \in R$.
    \begin{outline}
        \1 The \emph{domain} of $R$ is the set
        \[
            \text{dom}(R) = \set{ a \in A : (a, b) \in R \text{ for some } b \in B}
        \]
        \1 The \emph{range} of $R$ is the set
        \[
            \text{ran}(R) = \set{ b \in B : (a, b) \in R \text{ for some } a \in A}
        \]
        \1 $R$ is called a \emph{function} from $A$ to $B$, denoted as $R: A \to B$ iff
            \2 dom$(R) = A$
            \2 for each $a \in A$ there is (at most) one $b \in B$ such that $(a, b) \in R$.
    \end{outline}
\end{defn}

\textbf{Exercise:} Write definitions for \emph{injective} and \emph{surjective} functions.
\begin{rem}
    A \emph{bijective} function from $A$ to $B$ is an injective as well as surjective fuction from $A$ to $B$.
\end{rem}

\begin{axiom}[axiom of regularity] \label{zfc:regularity}
    Read up
\end{axiom}
\begin{axiom}[axiom of replacement] \label{zfc:replacement}
    Read up
\end{axiom}
\begin{axiom}[axiom of choice] \label{zfc:choice}
    Read up
\end{axiom}

\begin{defn} \label{defn:inductive}
    Given a set $A$, its \emph{successor} is the set \[
        A^{+} = A \cup \set{A}.
    \] A set $A$ is said to be \emph{inductive} if $\varnothing \in A$ and for every $a \in A$, we have $a^{+} \in A$.
\end{defn}
\begin{rem}
    The successor of $A$ is guaranteed to exist by \nameref{zfc:pairing} and \nameref{zfc:unions}. \\
    $\set{A}$ exists by \nameref{zfc:pairing} by letting $B = A$. \\
    $A \cup \set{A}$ exists by applying \nameref{zfc:unions} on the set $\set{A, \{A}\}$ formed using \nameref{zfc:pairing} again.

    $\set{A}$ can also be created as a subset (\nameref{zfc:specification}) of the power set (\nameref{zfc:powers}) of $A$.
\end{rem}
\begin{rem}
    The definition of an inductive set is very similar to the principle of mathematical induction in the Peano axioms.
\end{rem}

\begin{axiom}[axiom of infinity] \label{zfc:infinity}
    There exists an inductive set.    
\end{axiom}

\begin{lem} \label{lem:inductive intersection}
    Let $\mathscr{F}$ be a nonempty set of inductive sets. (This exists by \nameref{zfc:infinity} and \nameref{zfc:pairing}). Then \[
        \bigcap_{A \in \mathscr{F}}{A} \text{ is inductive.}
    \]
\end{lem}
\begin{proof}
    Assignment 1, Problem 5
\end{proof}

\begin{thm} \label{thm:omega}
    There exists a \emph{unique}, \emph{minimal} inductive set $\omega$, \textit{i.e.}, for any inductive set $S$, \[
        \omega \subseteq S
    \] and if $\omega'$ is any other inductive set satisfying this property, \[
        \omega = \omega'
    \]
\end{thm}
\begin{proof}[Proof under construction]
    Suppose there is no minimal inductive set. Then for any inductive set $\omega$, we have $ \omega \not\subseteq S$ for some inductive $S$. \\
    Thus there exists a set $\mathscr{F}$ of inductive sets, which contains $S$, such that $\bigcup_{A \in \mathscr{F}} A$ does not have $\omega$ as a subset.
\end{proof}


\begin{thm} \label{thm:omega peano}
    The $\omega$ in \cref{thm:omega} is a Peano set with successor function $a \mapsto a^{+}$.
\end{thm}

\begin{thm}[principle of recursion] \label{thm:recursion}
    Let $A$ be a set, and $f: A \to A$ be a function. Let $a \in A$. Then, there exists a function $F: \omega \to A$ such that
    \begin{enumerate}[label=(\alph*)]
        \item $F(\varnothing) = a$
        \item For some $b \in \omega$, we have $F(b^{+}) = f(F(b))$
    \end{enumerate}
\end{thm}

\textbf{Back to natural numbers!}

\subsection{Natural Numbers}

The ZFC axioms give us the existence of $\N = \set{0, 1, 2, \dots}$ with definitions as follows:
\begin{align*}
    0 &:= \varnothing \\
    1 &:= 0^{+} = \varnothing \cup \set{\varnothing} = \set{\varnothing} = \set{0} \\
    2 &:= 1^{+} = 1 \cup \set{1} = \set{0, 1} \\
    3 &:= 2^{+} = 2 \cup \set{2} = \set{0, 1, 2}
\end{align*}
This $\N = \omega$.

\begin{defn}[Peano addition] \label{defn:addn}
    Given a fixed $m \in \N$, the \nameref{thm:recursion} gives a unique function $\text{sum}_{m} : \N \to \N$
    \begin{enumerate}[label=(\alph*)]
        \item sum$_{m} (0) = m$ 
        \item sum$_{m} (n^{+}) = ($sum$_{m}(n))^{+}$
    \end{enumerate}
    Define \[
        m + n := \text{sum}_{m}(n)
    \]
\end{defn}

\begin{prop} \label{prop:2+3=5}
    $2 + 3 = 5$
\end{prop}
\begin{proof}
    \begin{align*}
        2 + 3 &= \text{sum}_{2}(3) \\
        &= \text{sum}_{2}(2^{+}) \\
        &= (\text{sum}_{2}(2))^{+} \tag{b}\\
        &= (\text{sum}_{2}(1^{+}))^{+} \\
        &= ((\text{sum}_{2}(1))^{+})^{+} \tag{b}\\
        &= ((\text{sum}_{2}(0^{+})^{+})^{+} \\
        &= (((\text{sum}_{2}(0))^{+})^{+})^{+} \tag{b}\\
        &= ((2^{+}))^{+})^{+} \tag{a}\\
        &= (3^{+})^{+} \\
        &= 4^{+} \\
        &= 5 \qedhere
    \end{align*}
\end{proof}

\begin{rem}
    Note that $m^{+} =$ sum$_{m}(0)^{+} = $ sum$_{m}(0^{+}) = $ sum$_{m}(1) = m + 1$. \\
    So we will now denote $m^{+}$ as $m + 1$.
\end{rem}

\begin{defn}[Peano multiplication] \label{defn:mult}
    Let $m \in \N$. By the recursion principle, $\exists$ a unique function
    \[
        \text{prod}_{m} : \N \to \N
    \]
    such that
    \begin{enumerate}[label=(\alph*)]
        \item prod$_{m}(0) = 0$
        \item prod$_{m}(n^{+}) = m + $ prod$_{m}(n)$
    \end{enumerate}
\end{defn}

\begin{thm} \label{thm:properties}
    The following hold:
    \begin{enumerate}[label=(\alph*)]
        \item\label{thm:comm} (Commutativity)
        \[
            m + n = n + m
        \]
        \[
            m \cdot n = n \cdot m
        \]
        for all natural numbers $m$ and $n$.

        \item\label{thm:asso} (Associativity)
        \[
            m + (n + k) = (m + n) + k
        \] \[
            m \cdot (n \cdot k) = (m \cdot n) \cdot k
        \]
        for all natural numbers $m, n, k$.

        \item\label{thm:dist} (Distributivity)
        \[
            m \cdot (n + k) = (m \cdot n) + (m \cdot k)
        \]
        
        \item\label{thm:m+n=0} $m + n = 0 \iff m = n = 0$ for any $m, n \in \N$

        \item $m \cdot n = 0 \iff m = 0$ or $n = 0$ for any $m, n \in \N$

        \item (Cancellation) $m + k = n + k \iff m = n$ for any $m, n, k \in \N$ and if $m \cdot k = n \cdot k$ and $k \neq 0$, then $m = n$.
    \end{enumerate}
\end{thm}

\begin{proof}
    \begin{enumerate}[label=(\alph*)]
        \item[(f)] We will prove $P(k)$ is true $\forall\; k \in \N$, where $P(k)$ is the property that for some fixed $m, n$, we have $m + k = n + k \iff m = n$ for all $m, n \in \N$ \\
        $P(0)$ is true as $m + 0 = m$, $n + 0 = n$, so $m + 0 = n + 0 \iff m + n$. \\
        Suppose $P(k)$ holds. Then $m + k^{+} = n + k^{+} \iff (m + k)^{+} = (n + k)^{+} \iff m + k = n + k$ (P3) which implies $m = n$ by $P(k)$. \\
        Since $m, n$ were arbitrary, $P(k)$ holds for any value of $m, n$.
    \end{enumerate}
\end{proof}

\subsubsection{Tao}
\begin{lem} \label{lem:n+0=n}
For any natural number $n$, $n+0=n$
\end{lem}

\begin{proof}
    Let $P(n)$ be the property that $n+0=n$. From \cref{defn:addn}, we have $0+0=0$, \emph{i.e.}, $P(0)$ is true. Suppose $P(n)$ is true for some natural number $n$. Then $n_{++}+0=(n+0)_{++}=n_{++}$, \emph{i.e.}, $P(n_{++})$ is true. By P5, we have $P(n)$ true for every natural number $n$.
\end{proof}

\begin{lem} \label{lem:n+succ(m)=succ(n+m)}
For any natural numbers $n$ and $m$, $n+m_{++}=(n+m)_{++}$
\end{lem}

\begin{proof}
    Let P(n, m) be the property
    \begin{equation*}
        n+m_{++}=(n+m)_{++}
    \end{equation*}
    $P(0, m)$ is clearly true. \\
    Suppose P(n, m) is true for some n. Then
    \begin{align*}
        n_{++}+m_{++} &= (n + m_{++})_{++}  \tag{\cref{defn:addn} (2)} \\
                      &= ((n+m)_{++})_{++}  \tag{induction hypothesis} \\
                      &= (n_{++}+m)_{++}    \tag{\cref{defn:addn} (2)}
    \end{align*}
    Therefore $P(n_{++}, m)$ holds. Since we made no assumptions on m, $P(n, m)$ holds for all $n, m$ by \cref{peano:induction}
\end{proof}

\begin{cor} \label{cor:succ(n)=n+1}
    $n_{++} = n + 1$
\end{cor}

\begin{proof}
    From \cref{lem:n+0=n,lem:n+succ(m)=succ(n+m)}, $n + 1 = n + 0_{++} = (n + 0)_{++} = n_{++}$.
\end{proof}

\begin{prop} \label{prop:addn is commutative}
\emph{(Addition is commutative)} For any natural numbers $n$ and $m$, $n + m = m + n$
\end{prop}

\begin{proof}
    Let $P(n, m) \iff n + m = m + n$
    \begin{align*}
        n_{++} + m &= (n + m)_{++} \tag{\cref{defn:addn} (2)} \\
                   &= (m + n)_{++} \tag{induction hypothesis} \\
                   &= m + n_{++}   \tag{\cref{lem:n+succ(m)=succ(n+m)}}
    \end{align*}
    Thus $P(n, m) \implies P(n_{++}, m)$ and we know $P(0, m)$ to be true (\cref{defn:addn} (1)) \\
    Since there were no assumptions on $m$, $P(n, m)$ is true for all $n$, $m$ by \cref{peano:induction}.
\end{proof}

\subsection{Fields, Ordered Sets and Ordered Fields}
We cannot solve
\begin{align*}
    3 + x &= 2 \\
    3x &= 2
\end{align*}
in $\N$.

\begin{defn} \label{defn:field}
    A field is a set $F$ with 2 operations $+ : F \times F \to F$ and $\cdot : F \times F \to F$ such that
    \begin{enumerate}[label=(F\arabic*)]
        \item \label{field:comm}
            $+$ \& $\cdot$ are commutative on $F$.
        \item \label{field:asso}
            $+$ \& $\cdot$ are associative on $F$.
        \item \label{field:dist}
            $+$ \& $\cdot$ satisfy distributivity on $F$, \textit{i.e.}, $a \cdot (b + c) = a \cdot b + a \cdot c$ for all $a, b, c \in F$. 
        \item \label{field:iden}
            There exist 2 \emph{distinct} elements, called 0 (additive identity) and 1 (multiplicative identity) such that
            \begin{align*}
                x + 0 &= x \\
                x \cdot 1 &= x
            \end{align*}
            for all $x \in F$
        \item \label{field:negative}
            For every $x \in F, \,\exists\; y \in F$ such that \[
                x + y = 0
            \]
        \item \label{field:reciprocal}
            For every $x \in F \setminus \set{0}, \,\exists\; z \in F$ such that \[
                x \cdot z = 1
            \]
    \end{enumerate}
\end{defn}
\begin{rem}
    We are tempted to call $y$ in \labelcref{field:negative} ``-x'' and $z$ in \labelcref{field:reciprocal} ``$\frac{1}{x}$'' but $y, z$ haven't been proven to be unique yet. \textcolor{red!85!black}{Prove this}. \textcolor{green!30!black}{Proved as \cref{lem:unique inverses}} \\
    Once we have proven this, we can also define $a - b := a + (-b)$ and $a/b = a \cdot \frac{1}{b}$.
\end{rem}

\begin{thm} \label{thm:0x=0}
    $(F, +, \cdot)$ is a field. Then for all $x$, \[
        0 \cdot x = x \cdot 0 = 0
    \]
\end{thm}
\begin{proof}
    By \labelcref{field:comm}, the first equality holds. \\
    Now by \labelcref{field:iden}, $1 + 0 = 1$. \\
    By \labelcref{field:dist}, $x \cdot (1 + 0) = x \cdot 1 + x \cdot 0$ \\
    So $x \cdot 1 = x \cdot 1 + x \cdot 0$ or $x = x + x \cdot 0$ \labelcref{field:iden}\\
    Adding $y$ to both sides where $x + y = 0$ \labelcref{field:negative} and using associativity and commutativity,
    \begin{align*}
        x + y &= x + x \cdot 0 + y \\ 
        0 &= x + y + x \cdot 0 \\
        &= 0 + x \cdot 0 \\
        &= x \cdot 0
    \end{align*}
    By commutativity, $0 \cdot x = 0$.
\end{proof}

\begin{defn} \label{defn:ordered set}
    A set $A$ with a relation $<$ is called an \emph{ordered set} if
    \begin{enumerate}[label=(O\arabic*)]
        \newcounter{temp}
        \item \label{order:trichotomy}
            (Trichotomy) For every $x, y \in A$, exactly one of the following holds. \[
                x < y, \quad x = y, \quad y < x
            \]
        \item \label{order:transitivity}
            (Transitivity) If $x < y$ and $y < z$, then $x < z$.
        \setcounter{temp}{\value{enumi}}
    \end{enumerate}
    \textbf{Notation:} $x < y$ is read as ``x is less than y'' \\
    $x \leq y$ means $x < y$ or $x = y$, read as ``x is less that or equal to y''. \\
    $x > y$ is read as ``$x$ is greater that $y$'' and equivalent to $y < x$.
\end{defn}
\textbf{Example:} $\N$ with $a < b$ iff $b = a + k$ for some non-zero $k$.

\begin{defn} \label{defn:ordered field}
    An \emph{ordered field} is a set that admits two operations $+$ and $\cdot$ and relation $<$ so that $(F, +, \cdot)$ is a field and $(F, <)$ is an ordered set and:
    \begin{enumerate}[label=(O\arabic*)]
        \setcounter{enumi}{\value{temp}}
        \item \label{order:addn preserves}
            For $x, y, z \in F$, if $x < y$ then $x + z < y + z$.
        \item \label{order:mult preserves}
            For $x, y \in F$, if $0 < x$ and $0 < y$ then $0 < x \cdot y$.
    \end{enumerate}
\end{defn}

\begin{lem} \label{lem:unique inverses}
    Given a field $(F, +, \cdot)$: For any element $a$ in a field $F$, there exists ony one $b$ such that $a + b = 0$. We will denote this $b$ as $-a$. Similarly for any $a$ in $F \setminus \set{0}$ there exists only one $b \in F$ such that $ab = 1$. We will denote this $b$ as $\frac{1}{a}$ or $a^{-1}$.
\end{lem}
\begin{proof}
    Suppose $a + b_{1} = 0$ and $a + b_{2} = 0$. Adding $b_{1}$ to both sides, we get $a + b_{2} + b_{1} = 0 + b_{1} \implies (a + b_{1}) + b_{2} = b_{1} \implies b_{2} = b_{1}$. \\
    The second part of the proof is analogous.
\end{proof}

\begin{lem} \label{lem:inverse involution}
    $-(-a) = a = (a^{-1})^{-1}$
\end{lem}
\begin{proof}
    $a + (-a) = 0$, so the additive inverse of $-a$ is $a$. \\
    $aa^{-1} = 1$, so the multiplicative inverse of $a^{-1}$ is $a$.
\end{proof}

\begin{lem} \label{lem:(-a)b=-(ab)}
    For any field $(F, +, \cdot)$, $(-a)b = -(ab)$ and $(-a)(-b) = ab$.
\end{lem}
\begin{proof}
    By the distributive law, we have $(a + (-a))b = ab + (-a)b \implies 0 = ab + (-a)b \implies (-a)b = -(ab)$. From this it follows that $(-a)(-b) = -(a(-b)) = -(-(ab))$
\end{proof}

\begin{thm} \label{thm:0<1}
    For any field $(F, +, \cdot)$, $0 < 1$.
\end{thm}
\begin{proof}
    By \cref{order:trichotomy} and \labelcref{field:iden}, either $0 < 1$ or $0 > 1$. \\
    If $0 < 1$, we are done. \\
    If $0 > 1$, then on adding $-1$ on both sides, $0 < -1$ by \cref{order:addn preserves}. \\
    So $0 < (-1)(-1) \iff 0 < 1$, a contradiction.
\end{proof}
\begin{rem}
    ``a contradiction'' is not necessary to state for the proof to be complete. See \href{https://teams.microsoft.com/l/message/19:5PNDOetYK3gbPZWX5Muk\_KnaEXgulRmRNwNmAHA8dZ81@thread.tacv2/1666960265828?tenantId=6f15cd97-f6a7-41e3-b2c5-ad4193976476&groupId=9cf683b7-9233-4d97-a7eb-1dc0051039a7&parentMessageId=1666960265828&teamName=UM\%20101\%20October\%202022&channelName=General&createdTime=1666960265828&allowXTenantAccess=false}{this discussion} at MS Teams.
\end{rem}

\subsection{Upper bounds \& least upper bounds}
Throughout this subsection, $(F, +, \cdot, <)$ is an ordered field, and we assume all ``basic'' properties. \\
\textbf{Key example:} $(\R, +, \cdot, <)$

\begin{defn} \label{defn:bounded above}
    A non-empty subset $S \subseteq F$ is said to be \emph{bounded above} in $F$ if there exists a $b \in F$ such that \[
        a \leq b \;\forall\; a \in S
    \]
    Here, $b$ is called an \emph{upper bound} of $S$. If $b \in S$, then $b$ is a \emph{maximum} of $S$.
\end{defn}
\begin{example}
    \begin{align*}
        S &= \set{ x \in F : 0 \leq x \leq 1 } \\
        T &= \set{ x \in F : 0 \leq x < 1 }
    \end{align*}
    Both $S$ and $T$ are bounded above as $1$ is an upper bound for both. \\
    $1$ is in fact, a maximum of S.
\end{example}
\begin{rem}
    If a maximum exists, it must be unique (\textcolor{red!85!black}{why?}).
\end{rem}
\begin{proof}
    Suppose $b_{1}, b_{2} \in S$ are two upper bounds of $S$. Then $b_{1} \leq b_{2}$ and $b_{2} \leq b_{1} \implies b_{1} = b_{2}$.
\end{proof}
\begin{rem}
    Upper bounds may not be unique.
\end{rem}

\begin{defn} \label{defn:supremum}
    Let $S \subseteq F$ be bounded above. An element $b \in F$ is said to be a \emph{least upper bound} of $S$ or a \emph{supremum} of $S$ if:
    \begin{enumerate}[label=(\alph*)]
        \item $b$ is an upper bound of $S$.
        \item If for $c \in F$, $c < b$, then $c$ is not an upper bound of $S$. In other words, for any $c < b, \;\exists\; s_{c} \in S$ such that $c < s_{c}$. \\
        Contrapositive: If $c$ is an upper bound of $S$, then $c$ is not less than $b$ $\iff c \geq b$.
    \end{enumerate}
\end{defn}
\begin{rem}
    There is only one supremum of $S$.
\end{rem}
\begin{proof}
    Suppose $b_{1}, b_{2} \in F$ are two supremums of $S$. Then since $b_{1}$ is an upper bound, $b_{1}$ is not less than $b_{2}$. Similarly $b_{2}$ is not less than $b_{1}$. By \labelcref{order:trichotomy}, $b_{1} = b_{2}$.
\end{proof}
\begin{example}
    \[
        \sup \set{x \in F : 0 \leq x < 1 } = 1
    \]
\end{example}
\begin{proof}
    Call the given set $T$. $T$ is non-empty as $0 \in T$. It is clear that $1$ is an upper bound (because $x < 1 \implies x < 1 \;\forall\; x \in T$). \\
    Let $a \in F$ s.t. $a < 1$. Now if $a < 0$, $a$ is not an upper bound as $0 \in T$. \\
    If $0 \leq a < 1$, first note that
    \begin{align*}
        0 < 1 &\leq a + 1 < 2 \\
        \implies 0 < \frac{1}{2} &\leq \frac{a+1}{2} < 1 \tag{\textcolor{red!85!black}{why?}}
    \end{align*}
    Thus, $\frac{a + 1}{2} \in T$.
    Since $a = \frac{a + a}{2} < \frac{a + 1}{2}$ (\textcolor{red!85!black}{why?}), $a$ is not an upper bound.
\end{proof}

\subsection{The Real Numbers}
We assume the existence of a set $\R$ with operations $+, \cdot$ and relation $<$ such that:
\begin{enumerate}[label=(\alph*)]
    \item $(\R, +, \cdot, <)$ is an ordered field.
    \item (LUB property) every non-empty bounded above subset in $\R$ has a supremum in $\R$.
\end{enumerate}

Some special subsets of $\R$:
\begin{itemize}
    \item $x > 0$ is called a positive real number.
    \item $x < 0$ is called a negative real number.
    \item $\N = \set{0, 1, 2, \dots}$ is a subset of $\R$ and inherits $+, \cdot, <$.
    \item $\P = \set{n \in \N : n \neq 0}$ is the set of positive natural numbers.
    \item $\Z = \N \cup \set{-n : n \in \P}$ is the set of integers.
    \item $\Q = \set{\frac{p}{q} : p \in \Z, q \in \P}$ is the set of rational numbers.
    \item $\Q' = \R \setminus \Q$ is the set of irrational numbers.
\end{itemize}

\begin{thm}[Archimedean property of $\R$] \label{thm:archimedean}
    Let $x, y \in \R$ and $x > 0$, then $\exists\; n \in \N$ such that \[
        n \cdot x > y
    \]
\end{thm}
\begin{proof}
    Assert $x > 0$. Let \[
        S = \set{nx : n \in \N}
    \]
    Suppose $S$ is bounded above. Since $x \in S$, $S$ is non-empty. Let $b$ be the supremum of $S$ (exists by LUB). $x > 0 \implies b - x < b$. Thus $b - x$ is not an upper bound of $S$, \textit{i.e.}, $\exists\; n \in \N$ such that $nx > b - x$. Thus $(n + 1)x > b$, and so $b$ is not an upper bound of $S$. Therefore $S$ cannot be bounded above. \\
    Hence for all $y$, $\exists\; s \in S$ such that $s > y$, and so the Archimedean property holds.
\end{proof}

\section{Sequences \& Series}
We will now assume \emph{everything} about real numbers: multiplication, division, exponentiation, etc.

\subsection{Sequences}
\begin{defn} \label{defn:sequence}
    A sequence in $\R$ is a function $f: \N \to \R$. We denote this sequence by $\set{a_{n}}_{n \in \N}$, where \[
        a_{n} = f(n) \quad \forall\; n \in \N
    \] and $a_{n}$ is called the $n^{th}$ term of $\set{a_{n}}_{n \in \N}$.
\end{defn}
\begin{rem}
    $\set{a_{n}} \subseteq \R$ will denote a sequence of real numbers. \\
    The numbering starts at 0 technically, but doesn't really matter. We will often omit the subscript $n \in \N$ and start indexing from some other point.
\end{rem}

\begin{defn} \label{defn:convergent seq}
    We say that a sequence $\set{a_{n}} \subseteq \R$ is \emph{convergent} (in $\R$) if $\exists\; L \in \R$ such that for each $\varepsilon > 0, \,\exists\; N_{\varepsilon,L} \in \N$ such that \[
        \abs{ a_{n} - L } < \varepsilon \quad \forall\; n \geq N_{\varepsilon, L}
    \] 
    We will call $L$ \emph{a} limit of $\set{a_{n}}$ and we write: \[
        a_{n} \to L \text{ as } n \to \infty
    \]
    A sequence $\set{a_{n}}$ is said to be \emph{divergent} if it is not convergent, \textit{i.e.}, $\forall\; L \in \R$ and $N_{L} \in \N$, $\exists\; \varepsilon > 0$ and $N \geq N_{L}$ such that \[
        \abs{ a_{N} - L } > \varepsilon
    \]
\end{defn}

\begin{thm}[Uniqueness of limits] \label{thm:unique limit}
    Suppose $L_{1}$ and $L_{2}$ are limits of a (convergent) sequence $\set{a_{n}} \in \R$. Then $L_{1} = L_{2}$.
\end{thm}
\begin{proof}[Proof \textcolor{red!70!black}{(self)}]
    Suppose $L_{1} \neq L_{2}$. Then define $\varepsilon = \frac{\abs{ L_{1} - L_{2} }}{2}$. There exists $N_{1}$ such that $\abs{ a_{n} - L_{1} } < \varepsilon \,\forall\; n \geq N_{1}$. So for all $n \geq N_{1}$,
    \begin{align*}
        \abs{ L_{1} - L_{2} }  &= \abs{ L_{1} - a_{n} + a_{n} - L_{2} }  \\
                                     &\leq \abs{ a_{n} - L_{1} } + \abs{ a_{n} - L_{2} } \\
                                     &\leq \varepsilon + \abs{ a_{n} - L_{2} }  \\
             \implies 2 \varepsilon  &\leq \varepsilon + \abs{ a_{n} - L_{2} } \\
                         \varepsilon &\leq \abs{ a_{n} - L_{2} } \qedhere
    \end{align*}
    % $\abs{ a_{n} - L_{1} } = \abs{ a_{n} - L_{2} + L_{2} - L_{1} } \leq \abs{ a_{n} - L_{2} } + \abs{ L_{2} - L_{1} }$
\end{proof}

\begin{example}
    \begin{enumerate}[label=(\alph*)]
        \item Let $\set{a_{n}} = \frac{1}{n^{p}} \,\forall\; n \in \P$, where $p > 0$. \[
            \lim_{n \to \infty} a_{n} = 0
        \]
        \begin{proof}
            Let $\varepsilon > 0$. \\
            By the Archimedean property of $\R$ applied to $x = \varepsilon^{\frac{1}{p}}$ and $y = 1$, $\exists\; N \in \P$ such that: \[
                N \varepsilon^{\frac{1}{p}} > 1 \implies \varepsilon^{\frac{1}{p}} > \frac{1}{N} \implies \varepsilon > \frac{1}{N^{p}}
            \]
            Let $n \geq N$. Then
            \begin{align*}
                \abs{ \frac{1}{n^{p}} - 0 } &= \frac{1}{n^{p}} \\
                                     &\leq \frac{1}{N^{p}} \\
                                     &< \varepsilon \qedhere
            \end{align*}
        \end{proof}

        \item $\set{(-1)^{n}}_{n \in \P}$ is divergent.
        \begin{proof}
            Suppose there exists a limit $L$. \\
            Let $\varepsilon = 1$. \\
            Then $\exists\; N \in \P$ such that $\abs{ a_{n} - L } < \varepsilon$ for all $n \geq N$. \\
            $\abs{ a_{2N} - L } < 1 \implies \abs{ L - 1 } < 1$. \\
            $\abs{ a_{2N + 1} - L } < 1 \implies \abs{ L + 1 } < 1$. \\
            $\abs{ 1 - L + L + 1 } \leq \abs{ 1 - L } + \abs{ L + 1 } < 2$ \\
            $\implies 2 < 2$. Contradiction.
        \end{proof}
    \end{enumerate}
\end{example}

\begin{defn} \label{defn:bounded seq}
    A sequence $\set{a_{n}}_{n \in \N}$ is said to be \emph{bounded} if $\exists\; M > 0$ such that $\abs{ a_{n} } < M \,\forall\; n \in \N$.
\end{defn}

\begin{thm} \label{thm:convergent=>bounded}
    Every convergent sequence is bounded.
\end{thm}
\begin{proof}
    Homework
\end{proof}

\begin{defn} \label{defn:monotone seq}
    A sequence $\set{a_{n}} \subseteq \R$ is said to be \emph{monotonically increasing} if $a_{n} \leq a_{n+1} \,\forall\; n \in \N$. \\
    A sequence $\set{a_{n}} \subseteq \R$ is said to be \emph{monotonically decreasing} if $a_{n} \geq a_{n+1} \,\forall\; n \in \N$. \\
    A sequence $\set{a_{n}} \subseteq \R$ is said to be \emph{monotone} if it is either monotonically increasing or monotonically decreasing.
\end{defn}

\begin{thm} \label{thm:MCT}
    A monotone sequence is convergent iff it is bounded.
\end{thm}
\begin{proof}
    Assume $\set{a_{n}}$ is increasing and $\exists\; M > 0$ such that $\abs{ a_{n} } < M \,\forall\; n \in \N$. \\
    Let $S = \set{ a_{n} : n \in \N }$. \\
    $S$ is nonempty. $S$ is bounded above. \\
    Thus, by LUB, there exists \[
        b = \sup S
    \] Let $\varepsilon > 0$. \\
    Since $a_{n} \leq b \,\forall\; n \in \N$, we have $a_{n} < b + \varepsilon$. \\
    Since $b$ is the lowest upper bound, $\exists\; N \in \N : a_{N} > b - \varepsilon$. \\
    Since $S$ is monotonically increasing, $b + \varepsilon > a_{n} \geq a_{N} > b - \varepsilon \;\forall\; n \geq N$. \\
    $\abs{ a_{n} - b } < \varepsilon \;\forall\; n \geq N$. Thus the given sequence is convergent, with limit $b$.

    A monotone sequence which is unbouded is divergent, as all convergent sequences are bounded (\cref{thm:convergent=>bounded}).
\end{proof}

\begin{rem}[Warning!]
    Divergent sequences may diverge for different reasons!
    \begin{itemize}
        \item $\set{(-1)^{n}}$ is bounded but divergent.
        \item $\set{n}$ is unbounded and divergent, to $+\infty$
        \item $\set{(-1)^{n}n}$ is unbounded and divergent, but not to $\pm \infty$.
    \end{itemize}
\end{rem}
\begin{defn} \label{defn:diverging to infinity}
    We say that a sequence diverges to $+\infty$ if $\forall\; R \in \R$, $\exists\; N_{R} \in \N$ such that $a_{n} > R \;\forall\; n \geq N_{R}$. \\
    We say that a sequence diverges to $-\infty$ if $\forall\; R \in \R$, $\exists\; N_{R} \in \N$ such that $a_{n} < R \;\forall\; n \geq N_{R}$. \\
    We write $\lim_{n \to \infty} a_{n} = +\infty$ or $\lim_{n \to \infty} a_{n} = -\infty$, but this is purely notational and does not mean ``$\set{a_{n}}$ has a limit''.
\end{defn}

\begin{thm}[Tao Theorem 6.1.19] \label{thm:}
    Suppose $\set{b_{n}}$ converges to $b \neq 0$ (and $\exists\; M \in \N$ such that $b_{n} \neq 0 \;\forall\; n \geq M$.) \\
    Then $\set{ \frac{1}{b} }_{n \geq M} \to \frac{1}{b}$ as $n \to \infty$.
\end{thm}
\begin{proof}[``Proof'']
    Given $\varepsilon > 0$, find $N \in \N$ such that \[
        \abs{ \frac{1}{b_{n}} - \frac{1}{b} } < \varepsilon \;\forall\; n \geq N
    \] 
    This is equivalent to \[
        \frac{\abs{ b - b_{n} }}{\abs{ b \cdot b_{n} } } < \varepsilon
    \]
    Since $b_{n} \to b$, there exists $M$ such that $\abs{ b_{n} - b } < \frac{b}{2}$ for all $n \geq M \implies \frac{b}{2} < b_{n} < \frac{3b}{2} \;\forall\; n \geq M$. \\
    Thus $\frac{1}{2}\abs{ b } < \abs{ b_{n} } < \frac{3}{2}\abs{ b } \implies \frac{2}{3\abs{ b }} < \frac{1}{\abs{ b_{n} }} < \frac{2}{\abs{ b }} \;\forall\; n \geq M$.

    Now for any $\varepsilon > 0$, there exists $M'$ such that $\abs{ b_{n} - b } <  \frac{2}{\abs{ b }^{2}} \varepsilon$ for any $n \geq M'$ \\
    $\implies \frac{\abs{ b_{n} - b }}{\abs{ b \cdot b_{n}}} < \frac{2 \abs{ b_{n} - b }}{\abs{ b}^{2}} < \varepsilon$ for all $n \geq M'$.
\end{proof}

\subsection{Infinite series}
\begin{defn} \label{defn:infinite series}
    An infinite series is a \emph{formal expression} of the form \[
        a_{0} + a_{1} + a_{2} + \dots, \text{ or }, \sum_{n=0}^{\infty} a_{n}
    \]
    Given $\sum_{n=0}^{\infty} a_{n}$, its sequence of partial sums (sops) is $\set{s_{n}}_{n=0}^{\infty}$ where
    \begin{align*}
        s_{0} &= a_{0} \\
        s_{1} &= a_{0} + a_{1} \\
        .\\
        .\\
        s_{n} &= a_{0} + a_{1} + \dots a_{n}
    \end{align*}
    We say that $\sum a_{n}$ is \emph{convergent} with sum $s$ if $\lim_{n \to \infty} s_{n} = s$. Otherwise, we say that $\sum a_{n}$ is divergent.
\end{defn}

\begin{example}
    \begin{enumerate}[label=(\alph*)]
        \item (Harmonic series) $\sum_{n=1}^{\infty} \frac{1}{n}$ is divergent.
        \begin{proof}
            $\set{s_{n}}$ is a monotonically increasing sequence.
            \begin{align*}
                s_{1} &= 1 \\
                s_{2} &= 1 + \frac{1}{2} \\
                s_{4} &= 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} > 1 + \frac{1}{2} + \frac{1}{4} + \frac{1}{4} \\
                s_{8} &= 1 + \frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{8} \\
                   &> 1 + \frac{1}{2} + 2 \cdot \frac{1}{4} + 4 \cdot \frac{1}{8} \\
                s_{2^{k}} &= 1 + \frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{2^{k}} \\
                   &> 1 + \frac{1}{2} + 2 \cdot \frac{1}{4} + \dots + 2^{k-1} \cdot \frac{1}{2^{k}} \\
                   &= 1 + \frac{k}{2}
            \end{align*}
            Thus, given any $R \in \R, \;\exists\; k \in \N$ such that $s_{2^{k}} > R$. \\
            $\implies \set{s_{n}}$ is divergent by MCT.
        \end{proof}
        \item $\sum_{n=1}^{\infty} \frac{1}{n^{2}}$ is convergent.
        \begin{proof}
            \begin{align*}
                s_{1} &= 1 \\
                s_{n} &= 1 + \sum_{k=2}^{n} \frac{1}{k^{2}} \\
                &< 1 + \sum_{k=2}^{n} \frac{1}{k(k-1)} \\
                &= 1 + \sum_{k=2}^{n} \paren{\frac{1}{k-1} - \frac{1}{k}} \\
                &= 1 + 1 - \frac{1}{n} \\
                &< 2 \;\forall\; n \in \N
            \end{align*}
            So $\set{s_{n}}$ is a monotonically increasing sequence that is bounded above. \\
            $\implies \set{s_{n}}$ is convergent.
        \end{proof}
    \end{enumerate}
\end{example}

\begin{rem}
    (Telescoping sum)
\end{rem}

\begin{thm} \label{thm:divergence test}
    Suppose $\sum a_{n}$ is convergent. Then \[
        \lim_{n \to \infty} a_{n} = 0
    \]
\end{thm}
\begin{proof}
    Suppose $\sum a_{n}$ converges to limit $L$. \\
    Then $\exists\; N \in \N$ such that for all $n \geq N$, $\abs{ s_{n} - L } < \frac{\varepsilon}{2} \;\forall\; \varepsilon > 0$.  \\
    Now $\abs{ s_{n+1} - s_{n} } = \abs{ s_{n+1} - L + L - s_{n} } \leq \abs{ s_{n+1} - L } + \abs{ s_{n} - L } < \varepsilon$ for all $n \geq N$. \\
    $\implies \abs{ a_{n} } < \varepsilon$ for all $n \geq N$, which implies $a_{n} \to 0$.
\end{proof}

\begin{example}[Geometric Series]
    Let $x \in \R$. Then \[
        \sum_{n=0}^{\infty} x^{n} = \begin{cases}
            \frac{1}{1-x} & \abs{ x} < 1 \\
            \text{diverges} & \abs{ x} \geq 1
        \end{cases}
    \]
\end{example}
\begin{proof}
    \begin{align*}
        s_{n} &= \paren{1 + x + x^{2} + \cdot x^{n} } \cdot \frac{1-x}{1-x} \\
        &= \frac{1-x^{n+1}}{1-x} \\
        &= \frac{1}{1-x} - \frac{x^{n+1}}{1-x}
    \end{align*}
    Claim: $x^{n}$ tends to 0 if $\abs{ x} < 1$ and diverges if $\abs{ x}  > 1$
    \begin{enumerate}[label=(Case \arabic*)]
        \item $\abs{ x} < 1$. Suffices to prove for $x > 0$. \\
        Note that $(1 + y)^{n} = 1 + ny + \dots > ny$. \\
        If $x < 1$, then $\frac{1}{x} > 1$. Say $\frac{1}{x} = 1 + y$ for some $y > 0$. \\
        Then \[
            \paren{\frac{1}{x}}^{n} = \paren{1 + y}^{n} > ny = n\paren{\frac{1}{x} - 1}
        \] So \[
            x^{n} < \frac{1}{n} c, \quad c = \frac{1}{\frac{1}{x} - 1}
        \] Since $\frac{1}{n}$ tends to $0$, $x^{n}$ tends to $0$ by the squeeze theorem (HW). By limit laws,
        \begin{align*}
            \lim_{n \to \infty} s_{n} &= \lim_{n \to \infty} \paren{\frac{1}{1-x} - \frac{x}{1-x}\cdot x^{n}} \\
            &= \frac{1}{1-x}
        \end{align*}
        
        \item $\abs{ x} \geq 1, x \neq 1$. $x = -1$ diverges as seen before. \\
        For $\abs{ x} > 1$, $\abs{ x} ^{n} = \paren{1 + (\abs{ x} - 1)}^{n} > n(\abs{ x} - 1)$. \\
        Once again, using the ``limit laws'', \[
            s_{n} = \frac{1}{1-x} - \frac{x}{1-x} \cdot x^{n} \text{ diverges}
        \]

        \item x = 1. $\sum (1)^{n}$ diverges since $a_{n} = 1^{n} \to 1 \neq 0$ as $n \to \infty$ \qedhere
    \end{enumerate}
\end{proof}

\begin{thm}[Comparison test] \label{thm:comparison}
    Suppose there exist constants $M \in \N$ and $0 < C$ such that \[
        0 \leq a_{n} \leq C b_{n} \quad\forall\; n \geq M
    \] If $\sum b_{n}$ converges, then $\sum a_{n}$ converges. In other words, If $\sum a_{n}$ diverges, $\sum b_{n}$ diverges.
\end{thm}
\begin{proof}
Let $\set{s_{n}}$ and $\set{t_{n}}$ be the suquence of partial sums of $\sum_{n=M}^{\infty} a_{n}$ and $\sum_{n=M}^{\infty} b_{n}$, respectively. Note that $\set{s_{n}}$ and $\set{t_{n}}$ are increasing sequences. By convergent of $\set{t_{n}}$, there exists $N \in\N$ and $L > 0$ such that \[
        t_{n} < L \quad \forall\; n \geq N
    \] Thus, $0 \leq s_{n} \leq Ct_{n} < CL \;\forall\; n \geq \max\set{M, N}$. \\
    Thus, $\set{s_{n}}$ is bounded and by monotone convergence theorem, $s_{n}$ converges. \\
    $\implies \sum a_{n}$ converges.
\end{proof}
\begin{example}
    Let $p \in \R$. Claim: \[
        \sum_{n=1}^{\infty} \frac{1}{n^{p}}
        \begin{cases}
            \text{converges} & p > 1 \\
            \text{diverges} & p \leq 1
        \end{cases}
    \]
\end{example}
\begin{proof}
    If $p \leq 0,$ check \[
        \paren{\frac{1}{n}} \not\to 0
    \] So $\sum \frac{1}{n^{p}}$ diverges.

    If $0 < p \leq 1$, then $n^{p} \leq n \;\forall\; n \geq 1$. Thus $\frac{1}{n^{p}} \geq \frac{1}{n}$. By the comparison test theorem, since $\sum \frac{1}{n}$ diverges, so does $\sum \frac{1}{n^{p}}$

    If $p \geq 2$, we have $\frac{1}{n^{p}} < \frac{1}{n^{2}}$. By the comparison test, this sum converges.

    Finally, if $1 < p < 2$. Note that
    \begin{align*}
        s_{1} &= 1 \\
        s_{3} &= 1 + \frac{1}{2^{p}} + \frac{1}{3^{p}} \leq 1 + 2 \cdot \frac{1}{2^{p}} \\
        s_{7} &= 1 + \frac{1}{2^{p}} + \frac{1}{3^{p}} + \dots \frac{1}{7^{p}} \leq 1 + 2\cdot \frac{1}{2^{p}} + 4 \cdot \frac{1}{4^{p}} \\
        s_{2^{k} - 1} &\leq 1 + \frac{2}{2^{p}} + \dots + \frac{2^{k-1}}{2^{p(k-1)}} \\
        &= 1 + \frac{1}{2^{p-1}} + \frac{1}{2^{2(p-1)}} + \dots + \frac{1}{2^{(k-1)(p-1)}}
    \end{align*}
    The RHS are sops of \[
        \sum_{n=0}^{\infty} \paren{\frac{1}{2^{p-1}}}^{n} = \frac{1}{1-\paren{\frac{1}{2}}^{p-1}}
    \] So, $s_{2^{k}-1} < \frac{1}{1-\paren{\frac{1}{2}}^{p-1}} \;\forall\; k \in \P$ and $s_{n}$ is increasing. \\
    Thus by ``MCT'', $\set{s_{n}}$ converges. (\textcolor{red!85!black}{prove $\set{s_{n}}$ is bounded})
\end{proof}

\begin{thm}[Ratio test] \label{thm:ratio test}
    Let $\sum a_{n}$ be a series of positive terms. Suppose \[
        \lim_{n \to \infty} \frac{a_{n+1}}{a_{n}} = L \in \R
    \] Then,
    \begin{enumerate}[label=(\alph*)]
        \item If $L < 1$, the series converges.
        \item If $L > 1$, the series diverges.
        \item If $L = 1$, the test is inconclusive.
    \end{enumerate}
\end{thm}
\begin{proof}[Proof \textcolor{red!70!black}{(self)}]
    \begin{enumerate}[label=(\alph*)]
        \item There exists an $N \in \N$ such that $\frac{a_{n+1}}{a_{n}} < (L + \varepsilon) \;\forall\; n \geq N$. Thus $a_{N+k} < (L + \varepsilon)^{k} a_{N} \;\forall\; k > 0$. Since $L < 1$, choose $\varepsilon = \frac{1 - L}{2} \implies (L + \varepsilon) < 1$. Since the geometric series converges, so does $\sum a_{n}$ by the comparison test.
        \item There exists an $N \in \N$ such that $\frac{a_{n+1}}{a_{n}} > (L - \varepsilon) \;\forall\; n \geq N$. Thus $a_{N+k} > (L + \varepsilon)^{k} a_{N} \;\forall\; k > 0$. Since $L > 1$, choose $\varepsilon = \frac{L - 1}{2} \implies (L - \varepsilon) > 1$. Since the geometric series diverges, so does $\sum a_{n}$ by the comparison test (contrapositive).
        \item $\sum a_{n}$ diverges for $a_{n} = \frac{1}{n}$, converges for $a_{n} = \frac{1}{n^{2}}$. Both these sequences satisfy $L = 1$, thus the test is inconclusive.
    \end{enumerate}
\end{proof}

\begin{thm} \label{thm:series limit laws}
    Suppose $\sum a_{n}$ and $\sum b_{n}$ converge with sums $a$ and $b$ respectively. Then, for constants $l$ and $m$, $\sum la_{n} + mb_{n}$ converges to $la + mb$. Suppose $\sum \abs{ a_{n}}$ and $\sum \abs{ b_{n}}$ converge. Then, so does $\sum \abs{ la_{n} + mb_{n} }$ for any choice of $l$ and $m$ in $\R$.
\end{thm}
\begin{proof}[Proof \textcolor{red!70!black}{(self)}]
    Let $s_{n}$ and $t_{n}$ be the sops of $a_{n}$ and $b_{n}$. Let $S_{n}$ and $T_{n}$ be the sops of $la_{n}$ and $mb_{n}$. We have $S_{n} = l s_{n}$ and $T_{n} = m t_{n}$, so by the limit laws for sequences we have $\sum la_{n} = l \sum a_{n}$ and $\sum mb_{n} = m \sum b_{n}$. \\
    By the limit laws for sequences and defining $Q_{n} =$ sops of $la_{n} + mb_{n} = S_{n} + T_{n}$, we have $\sum la_{n} + mb_{n} = l \sum a_{n} + m \sum b_{n}$.

    Now suppose $\sigma_{n}, \tau_{n}$ and $\mu_{n}$ are the sops of $\abs{ a_{n}}, \abs{ b_{n}}$ and $\abs{ a_{n} + b_{n}}$. Since $\abs{ a_{n} + b_{n}} \leq \abs{ a_{n}} + \abs{ b_{n}}$, we have $\mu_{n} \leq \sigma_{n} + \tau_{n}$. Since $\sigma$ and $\tau$ are monotone and convergent, they are bounded. So $\mu_{n} \leq \sup \set{\sigma_{n}} + \sup \set{\tau_{n}}$. Thus $\mu$ is bounded. By MCT, it is convergent.
\end{proof}

\begin{cor} \label{cor:}
    Suppose $\sum a_{n}$ converges and $\sum b_{n}$ diverges. Let $m \in \R \setminus \set{0}$. Then, $\sum(a_{n} + b_{n})$ diverges, and $\sum mb_{n}$ diverges.
\end{cor}

\begin{defn} \label{defn:absolute convergence}
    A series $\sum a_{n}$ of real numbers is said to \emph{converge absolutely} if $\sum \abs{ a_{n}}$ converges. A series $\sum a_{n}$ of real numbers is said to \emph{converge conditionally} if $\sum \abs{ a_{n}}$ diverges but $\sum a_{n}$ converges.
\end{defn}
\begin{thm} \label{thm:}
    If $\sum a_{n}$ converges absolutely, it must converge. Moreover, $\abs{ \sum a_{n}} \leq \sum \abs{ a_{n}}$.
\end{thm}
\begin{proof}
    We construct a new series \[
        b_{n} = a_{n} + \abs{ a_{n}}
    \] Observe that $0 \leq  b_{n} \leq  2 \abs{  a_{n}  }$. Thus, by the comparison test, $\sum b_{n}$ converges. Now, by the limit laws for convergent series, $\sum a_{n} = \sum (b_{n} - \abs{ a_{n}})$ converges.
\end{proof}

\begin{example}
    $\sum \frac{(-1)^{n}}{n}$ is convergent.
\end{example}
\begin{proof}
    \begin{align*}
        s_{1} &= -1 \\
        s_{3} &= -1 + \paren{\frac{1}{2} - \frac{1}{3}} > s_{1} \\
        s_{5} &= s_{3} + \paren{\frac{1}{4} - \frac{1}{5}} > s_{3} \\
        \vdots \\
        s_{2k+1} &= \paren{-1 + \frac{1}{2}} + (-\frac{1}{3} + \frac{1}{4}) + \dots \paren{-\frac{1}{2k+1}} < 0
    \end{align*}
    Thus, $\set{s_{2k+1}}$ being a bounded increasing sequence, converges to some limit $l$.
    \begin{align*}
        s_{2} &= -1 + \frac{1}{2} \\
        s_{4} &= -1 + \frac{1}{2} - \paren{\frac{1}{3} - \frac{1}{4}} < s_{2} \\
        \vdots \\
        s_{2k} &= -1 + \paren{\frac{1}{2} - \frac{1}{3}} + \paren{\frac{1}{4} - \frac{1}{5}} + \dots + \paren{\frac{1}{2k}} > -1
    \end{align*}
    Thus, $\set{s_{2k}}$ being a bounded decreasing sequence, converges to some limit $m$. Moreover, $s_{2k+1} = s_{2k} - \frac{1}{2k + 1}$. So by limit laws for sequences, $l = m$. \textcolor{red!80!black}{Why does this suffice to claim that $\set{s_{n}}$ converges?}.

    For any $\varepsilon > 0$, there exist $n_{1}, n_{2}$ such that $\abs{ s_{2k} - l } < \varepsilon \;\forall\; 2k \geq n_{1}$ and $\abs{ s_{2k+1} - l } < \varepsilon \;\forall\; 2k + 1 \geq  n_{2}$. Choose $N = \max \set{n_{1}, n_{2}}$. Then $\abs{ s_{n} - l } < \varepsilon$ for all $n \geq  N$.
\end{proof}

\begin{thm}[Alternating Series Test/Leibniz Test] \label{thm:leibniz test}
    Suppose $\set{a_{n}}$ is a decreasing sequence of positive numbers going to 0. Then, $\sum (-1)^{n} a_{n}$ converges. Denoting the sum by $S$, we have that \[
        0 < (-1)^{n}(S - s_{n}) < a_{n+1}
    \]
\end{thm}
\begin{proof}
    Same principle as the example of $\sum \frac{(-1)^{n}}{n}$.

    $a_{2k} > a_{2k+1} \implies (-1)^{2k} a_{2k} + (-1)^{2k+1} a_{2k+1} > 0 \implies s_{2k+1} > s_{2k-1}$. So $\set{s_{2k+1}}$ is increasing. $s_{2k+1} > -a_{1}$. Also $s_{2k+1} = s_{2k} - a_{2k+1} < s_{2k} < 0$ (see below).
    \\\\
    $a_{2k+1} > a_{2k+2} \implies (-1)^{2k+1} a_{2k+1} + (-1)^{2k+2} a_{2k+2} < 0 \implies s_{2k} > s_{2k+2}$. So $\set{s_{2k}}$ is decreasing. $s_{2k} < -a_{1} + a_{2} < 0$. Also $s_{2k} = s_{2k-1} + a_{2k} > -a_{1} + a_{2k} > -a_{1}$.
    \\\\
    Thus $\set{s_{2k}}$ and $\set{s_{2k-1}}$ are bounded and monotone, thus convergent. Since $s_{2k} = s_{2k-1} + a_{2k}$, by limit laws we have $\lim s_{2k} = \lim s_{2k-1}$. Thus the sequence converges.
\end{proof}

\begin{rem}
    The estimate is AST allows us to estimate sums of alternating series within any prescribed error. For instance, to know $\sum_{n=1}^{\infty} \frac{(-1)^{n}}{n}$ up to an error of $0.01$. I need to find $n$ so that \[
        \abs{ S - s_{n} } < \frac{1}{100}.
    \]
    Take $n = 99$, or the sum of the first $99$ terms.
\end{rem}


\section{Limits \& Continuity}
\subsection{Limit of a function}
\begin{example}
    \begin{enumerate}[label=(\alph*)]
        \item For $f(x) = c$, $c \in \R$, \[
            \lim_{x \to p} f(x) = c
        \] Choose $\delta = 1$. $0 < \abs{ x - p } < \delta \implies \abs{ f(x) - c } = 0 < \varepsilon \;\forall\; \varepsilon > 0$.

        \item For $f(x) = x$, \[
            \lim_{x \to p} f(x) = p
        \] Choose $\delta = \varepsilon$. $ 0 < \abs{ x - p } < \delta \implies \abs{ f(x) - p} < \varepsilon$.

        \item For $f(x) = \sqrt{x}$ and $p > 0$, \[
            \lim_{x \to p} f(x) = \sqrt{p}
        \]
        \begin{align*}
            \abs{ \sqrt{x} - \sqrt{p} } &< \varepsilon \\
            \iff \frac{\abs{ x - p }}{\abs{ \sqrt{x} + \sqrt{p} }} &< \varepsilon \\
        \end{align*}
        Take $\delta = \min \set{p, \sqrt{p} \varepsilon}$ (this is to make sure $f$ is defined for all points in $N_{\delta}(p) \setminus \set{p}$). Now $\abs{ x - p} < \delta \implies$
        \begin{align*}
            \frac{\abs{ x - p }}{\abs{ \sqrt{x} + \sqrt{p} }} &< \frac{\delta}{\abs{ \sqrt{x} + \sqrt{p} }} \\
            &= \frac{\sqrt{p} \varepsilon}{\abs{ \sqrt{x} + \sqrt{p} }} \\
            &< \varepsilon \qedhere
        \end{align*}

        \item For $f(x) = \frac{1}{x}, x \neq 0$, \[
            \lim_{x \to 0} f(x) \text{ does not exist}
        \]
        \begin{proof}
            Suppose $\exists\; L \in \R$ such that \[
                \lim_{x \to 0} f(x) = L
            \] Choose $\varepsilon = \frac{1}{L}$. Then $\exists\; \delta > 0$ such that $ 0 < \abs{ x - 0 } < \delta \implies \abs{ f(x) - L } < 1$. By the Archimedean property, $\exists\; N \in \N$ such that $\frac{1}{N} < \delta$.

            Now $0 < \frac{1}{N+2} < \frac{1}{N} < \delta$. Thus by our hypothesis, $\abs{ N + 2 - L } < 1$ and $\abs{ N - L } < 1 \implies \abs{ 2 } < \abs{ N + 2 - L } + \abs{ N - L } < 1 + 1 = 2$. Contradiction.
        \end{proof}
    \end{enumerate}
\end{example}

\begin{thm}[Limit laws for functions] \label{thm:func limit laws}
    Suppose $f$ and $g$ are functions such that \[
        \lim_{x \to p} f(x) = a, \qquad \lim_{x \to p} g(x) = b.
    \] Then,
    \begin{align}
        \lim_{x \to p} (f \pm g)(x) &= a \pm b \\
        \lim_{x \to p} (f \cdot g)(x) &= a \cdot b \\
        \lim_{x \to p} (f/g)(x) &= a/b
    \end{align}
\end{thm}
\begin{proof}
    Scrapwork: $\abs{ f(x) g(x) - ab } = \abs{ f(x)g(x) - f(x) b + f(x) b - ab } \leq  \abs{ f(x) } \abs{ g(x) - b } + \abs{ b } \abs{ f(x) - a }$.
    \\\\
    Let $\varepsilon > 0$. Since $\lim_{x \to p} f(x) = a$, corresponding to $\varepsilon_{1} = \frac{\varepsilon}{2(\abs{ b } + 1)} \;\exists\; \delta_{1} > 0 $ such that if $0 < \abs{ x - p } < \delta_{1}$, we have $\abs{ f(x) - a } < \frac{\varepsilon}{2(\abs{ b } + 1)}$. \\
    Also, \begin{align*}
        \abs{ f(x) } - \abs{ a} &\leq  \abs{ f(x) - a } \\
        &< \varepsilon_{1} \\
        \implies \abs{ f(x) } &< \abs{ a } + \varepsilon_{1} =: M_{\varepsilon} > 0
    \end{align*}
    Let $\varepsilon_{2} = \frac{\varepsilon}{2M}$. Since $\lim_{x \to p} g(x) = b$, $\exists\; \delta_{2} > 0$ such that if $0 < \abs{ x - p } < \delta_{2}$, then $\abs{ g(x) - b } < \varepsilon_{2}$.
    \\\\
    Let $\delta = \min\set{\delta_{1}, \delta_{2}}$. Then,
    \begin{align*}
        \abs{ f(x) g(x) - ab } &\leq \abs{ f(x) } \abs{ g(x) - b} + \abs{ b } \abs{ f(x) - a } \\
        &< M_{\varepsilon} \cdot \frac{\varepsilon}{2 M_{\varepsilon}} + \abs{ b } \cdot \frac{\varepsilon}{2(\abs{ b} + 1)} \\
        &< \varepsilon \qedhere
    \end{align*}
\end{proof}
\begin{proof}[Mrigank's trick]
    \begin{align*}
        \abs{ f(x) g(x) - ab } &= \abs{ (fg - ag - bf + ab) + (ag - ab) + (bf - ab) } \\
        &\leq \abs{ f(x) - a } \abs{ g(x) - b } + \abs{ a } \abs{ g(x) - b } + \abs{ b }\abs{ f(x) - a }
    \end{align*}
    Choose $\delta$ such that $\abs{ g(x) - b }$ and $\abs{ f(x) - b } < \min \set{\sqrt{\frac{\varepsilon}{3}}, \frac{\varepsilon}{3\abs{ a}}, \frac{\varepsilon}{3\abs{ b}}}$. Then
    \begin{align*}
        \abs{ f(x) g(x) - ab } &< \sqrt{\frac{\varepsilon}{3}} \sqrt{\frac{\varepsilon}{3}} + \abs{ a } \cdot \frac{\varepsilon}{3\abs{ a}} + \abs{ b} \cdot \frac{\varepsilon}{3 \abs{ b}} \\
        &= \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} \\
        &= \varepsilon \qedhere
    \end{align*}
\end{proof}

\subsection{Continuity}
\begin{defn} \label{defn:continuous}
    Let $S \subseteq \R$ be a (nonempty) subset, $f: S \to \R$ and $p \in S$. We say that $f$ is continuous at $p$ iff: \\
    for every $\varepsilon > 0, \;\exists \delta_{\varepsilon} > 0$ such that \[
        \abs{ x - p } < \delta_{\varepsilon} \;\land\; x \in S \implies \abs{ f(x) - f(p) } < \varepsilon
    \] We say that $f$ is continuous on $S$ iff $f$ is continuous at each $p \in S$.
\end{defn}
\begin{rem}
    It is possible that $\exists\; \delta$ such that $N_{\delta}(p) \cup S = \set{p}$. \textit{E.g.}, $S = \N, p = 0, \delta \leq 1$
\end{rem}
\begin{rem}
    If $f$ is defined on some interval $(a, b)$ containing $p$, then this definition is equivalent to \[
        \lim_{x \to p} f(x) = f(p)
    \] \textcolor{red!85!black}{How?} For any $\varepsilon > 0$, choose $\delta = \min \set{\delta_{\varepsilon}, b - p, p - a}$. Then $f$ is defined on all points in $N_{\delta}(p)$, and for all $x \in N_{\delta}(p)$, we have $f(x) \in N_{\varepsilon}(f(p))$. Thus \[
        \lim_{x \to p} f(x) = f(p)
    \]
\end{rem}

\begin{thm}[Algebraic laws for continuity] \label{thm:continuity laws}
    Suppose $f$ and $g$ are continuous at $p \in S$. Then so are $f \pm g, fg$ and if $g(p) \neq 0, f/g$.
\end{thm}
\begin{proof}
    For any $\varepsilon > 0$,
    \begin{enumerate}[label=(\alph*)]
        \item[($f \pm g$)] there exist $\delta_{f}, \delta_{g}$ such that $\abs{ x - p } < \min \set{\delta_{f}, \delta_{g}} \;\land\; x \in S \implies \abs{ f(x) - f(p) }, \abs{ g(x) - g(p) } < \frac{\varepsilon}{2}$. Thus we have \[
            \abs{ (f \pm g)(x) - (f \pm g)(p) } < \abs{ f(x) - f(p) } + \abs{ g(x) - g(p) } < \varepsilon
        \] 
        \item[$(fg)$] there exist $\delta_{f}, \delta_{g}$ such that $\abs{ x - p } < \min \set{\delta_{f}, \delta_{g}} \;\land\; x \in S \implies \abs{ f(x) - f(p) }, \abs{ g(x) - g(p) } < \min\set{\frac{\varepsilon}{3g(p)}, \frac{\varepsilon}{3f(p)}, \sqrt{\frac{\varepsilon}{3}}}$ (handle $fg = 0$). Then we have
        \begin{align*}
            \abs{ fg (x) - fg (p) } &= \abs{ (f(p) + f(x) - f(p))(g(p) + g(x) - g(p)) - fg(p) } \\
            &= \abs{ f(p) (g(x) - g(p)) + g(p) (f(x) - f(p)) + (f(x) - f(p))(g(x) - g(p)) } \\
            &\leq \abs{ f(p) } \abs{ g(x) - g(p) } + \abs{ g(p) } \abs{ f(x) - f(p)} + \abs{ f(x) - f(p) } \abs{ g(x) - g(p) } \\
            &< \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} \\
            &= \varepsilon
        \end{align*}

        \item[$(f/g)$] Let $h = \frac{1}{g}$. Since $g$ is continuous, $\exists\; \delta_{0}$ such that \[
            \abs{ x - p } < \delta_{0} \;\land\; x \in S \implies \abs{ g(x) - g(p) } < \frac{\abs{ g(p)}}{2} \implies \abs{ g(x) } < \frac{3}{2} \abs{ g(p) }
        \] Now, choose $\delta = \min \set{\delta_{0}, \frac{3}{2} \abs{ g(p)}^{2}}$. For $\abs{ x - p } < \delta$, \[
            \abs{ g(x) - g(p) } = \abs{ g(x) } \abs{ g(p) } \abs{ \frac{1}{g(x)} - \frac{1}{g(p)} } < \frac{3}{2} \abs{ g(p) }^{2} \abs{ \frac{1}{g(x)} - \frac{1}{g(p)} } < \delta \leq \frac{3}{2} \abs{ g(p) }^{2}
        \] \[
            \implies 
        \]
    \end{enumerate}
\end{proof}

\begin{thm} \label{thm:compositions}
    Let $f: A \to \R$ and $g: B \to \R$ be continuous functions such that $f(A) = \mathop{range}(f) \subseteq B$. Then, \[
        g \circ f(x) = g(f(x)) : A \to \R
    \] is continuous.
\end{thm}

\begin{proof}
    Let $p \in A$ and $q = f(p)$. Let $\varepsilon > 0$. Since $g$ is continuous at $q$, $\exists\; \tau > 0$ such that whenever $y \in B$ and $\abs{ y - q } < \tau$, then $\abs{ g(y) - g(q) } < \varepsilon$.

    Let $\varepsilon_{1} = \tau$. Then by the continuity of $f$ at $p$, $\exists\; \delta > 0$ such that whenever $x \in A \land \abs{ x - p } < \delta$, we have $\abs{ f(x) - f(p) } < \varepsilon_{1} = \tau$.

    Thus, $\abs{ g(f(x)) - g(f(p)) } < \varepsilon$. Since $\varepsilon > 0$ and $p \in A$ were arbitrary, $g \circ f$ is continuous.
\end{proof}

\begin{thm}[Intermediate Value Theorem] \label{thm:intermediate value}
    Let $f: [a, b] \to \R$ be a continuous function. Suppose $y \in \R$ is a number between $f(a)$ and $f(b)$, \textit{i.e.}, $y \in [f(a), f(b)]$. Then $\exists\; c \in [a, b]$ such that \[
        f(c) = y
    \]
\end{thm}
\begin{cor}[Bolzano] \label{cor:bolzano}
    Let $f : [a, b] \to \R$ be a continuous function such that $f(a)$ and $f(b)$ take opposite signs. Then $\exists\; c \in (a, b)$ such that $f(c) = 0$.
\end{cor}
\begin{rem}
    Bolzano's statement is equivalent to the IVT (let $g = f - y$).
\end{rem}

\begin{thm}[The Borsuk-Ulam Theorem : baby version] \label{thm:borsuk-ulam}
    Let $S$ be the set $\set{(x, y) \in \R \times \R : x^{2} + y^{2} = 1}$. Let $f: S \to \R$ be a continuous function. Then there exists a pair of antipodal points on the circle which have the same value of $f$.
\end{thm}

\begin{proof}["Proof"]
    A continuous function $f$ on $S$ is a $2\pi$-periodic function on $\R$.
    
    View $f$ on $[0, \pi]$. Let \[
        g(\theta) = f(\theta) - f(\theta + \pi)
    \] $g$ is continuous (Note that $f(\theta + \pi)$ is continous as it can be viewed as a composition). \\
    Then either $g(0) = g(\pi) = 0$, or $g(0) = -g(\pi) \neq 0$. Thus there exists $c \in (0, \pi)$ such that $f(c) = 0$.
\end{proof}

\begin{lem} \label{lem:comparison of seq}
    Let $a_{n}, b_{n}$ be convergent sequences such that $a_{n} \leq b_{n}$ for all $n$ (large enough). Then \[
        \lim_{n \to \infty} a_{n} \leq \lim_{n \to \infty} b_{n}
    \]
\end{lem}
\begin{proof}
    Let $c_{n} = a_{n} - b_{n}$. By limit laws, $\lim_{n \to \infty} c_{n}$ exists. Since $c_{n} > 0 \;\forall\; n \geq N$, $\lim_{n \to \infty} c_{n} \geq 0$ (if it were negative, choose $\varepsilon = L$ giving $c$ negative). This gives \[
        \lim_{n \to \infty} a_{n} \leq \lim_{n \to \infty} b_{n} \qedhere
    \]
\end{proof}

\begin{proof}[Proof of IVT]
    We may assume: $y$ is strictly between $f(a)$ and $f(b)$. Further, we may assume $f(a) < y < f(b)$. The other case is \textcolor{red!75!black}{HW}. Let \[
        S = \set{x \in [a, b]: f(x) < y}.
    \] $S$ is nonempty as $a \in S$. Moreover, $b$ is an upper bound of $S$. \\
    $\implies c := \sup S$ exists. 

    Let $n \in \P$. Then $c - \frac{1}{n}$ is not an upper bound of $S$. Therefore $\exists\; x_{n} \in S$ such that \[
        c - \frac{1}{n} < x_{n} \leq c \qquad f(x_{n}) < y.
    \] By squeeze theorem, $\set{x_{n}} \to c$ as $n \to \infty$. By the sequential character of continuity, \[
        \lim_{n \to \infty} f(x_{n}) = f(c)
    \] But $f(x_{n}) < y \;\forall\; n \in \N$. By \cref{lem:comparison of seq}, $\lim_{n \to \infty} f(x_{n}) \leq y \implies f(c) \leq y$. 

    Similarly, defining $x_{n} = c + \frac{1}{n}$ yields $f(c) \geq y$. And thus we have \[
        f(c) = y \qedhere
    \]
\end{proof}

\begin{proof}[Proof \textcolor{red!70!black}{(self)}]
    $\implies c := \sup S$ exists.

    Suppose $f(c) < y$. $f(b) > y > f(c) \implies c \neq b$. There exists $\delta > 0$ such that $\abs{ x - c } < \delta \implies \abs{ f(x) - f(c) } < \abs{ f(c) - y } \implies f(x) < y$. Thus $c + \delta/2 \in S$, a contradiction.

    Now suppose $f(c) > y$. $f(a) < y < f(c) \implies c \neq b$. There exists $\delta > 0$ such that $\abs{ x - c } < \delta \implies \abs{ f(x) - f(c) } < \abs{ f(c) - y } \implies f(x) > y$. Thus $c - \delta/2 \not\in S$, a contradiction.

    So $f(c)$ must be equal to $y$.
\end{proof}

\begin{defn} \label{defn:bounded fn}
    A function $f : S \to \R$ is said to be \emph{bounded above} on $S$ if $\exists\; U \in \R$ such that $f(x) \leq U \;\forall\; x \in S$.

    $f$ is said to be \emph{bounded} if $\exists\; M > 0$ such that $\abs{ f(x) } < M \;\forall\; x \in S$.
\end{defn}

\begin{thm}[Continuous functions on closed, bounded intervals are bounded.] \label{thm:closed bounded}
    Let $f : [a, b] \to \R$ be continuous on $[a, b]$. Then $f$ is a bounded function.
\end{thm}
\begin{proof}[Proof by contradiction \textcolor{red!70!black}{(morphed)}]
    Let $a_{0} = a$, $b_{0} = b$, and $I_{n} = [a_{n}, b_{n}]$. If $f$ is not bounded on $I_{n}$, then it is not bounded on at least one of $[a_{n}, \frac{a_{n} + b_{n}}{2}]$ and $[\frac{a_{n} + b_{n}}{2}, b_{n}]$. Let the subinterval on which it is unbounded be $I_{n+1}$ (choose the former if it is unbounded on both). Since $f$ is unbounded on $I_{0}$, it is unbounded on $I_{k} \;\forall\; k \in \N$. Note that $\abs{ I_{n+1} } = \frac{1}{2} \abs{ I_{n} }$. Thus $\abs{ I_{n} } = \frac{1}{2^{n}} \abs{ I_{0} } = \frac{1}{2^{n}} (b - a)$. Also note that $a_{n+1} \geq a_{n}$.

    Let $a_{\infty}$ be the supremum of $\set{a_{n}}$. Note that $b$ is an upper bound of $a_{n}$, so $a_{\infty} \leq b$. Also since $a \in \set{a_{n}}$, $a \leq a_{\infty}$.

    Note that $p, q \in N_{\delta}(a_{\infty}) \cap [a, b]$ with $p < q$ implies $[p, q] \subseteq N_{\delta}(a_{\infty}) \cap [a, b]$.

    By continuity $\exists\; \delta > 0$ such that $\abs{ f(x) - f(a_{\infty}) } < 1 \;\forall\; x \in N_{\delta}(a_{\infty}) \cap [a, b]$. That is, $f$ is bounded on $N_{\delta}(a_{\infty}) \cap [a, b]$.

    There exists a $k \in \N$ such that $a_{\infty} - \delta < a_{k} \leq a_{\infty}$. Since $\set{a_{n}}$ is increasing, we have $a_{\infty} - \delta < a_{n} \leq a_{\infty} \;\forall\; n \geq k$. Choose $N$ such that $\frac{b - a}{2^{n}} = \abs{ I_{n} } < \delta \;\forall\; n \geq N$.

    Letting $n_{0} = \max \set{k, N}$, we get $a_{\infty} - \delta < a_{n_{0}} \leq b_{n_{0}} < a_{\infty} + \delta$. Thus $[a_{n_{0}}, b_{n_{0}}] \subseteq N_{\delta}(a_{\infty}) \cap [a, b] \implies I_{n_{0}}$ is bounded, a contradiction. 
\end{proof}

\begin{defn} \label{defn:global extrema}
    A function $f : S \to \R$ is said to have a \emph{global maximum} on $S$ at a point $p \in S$ if $f(x) \leq f(p) \;\forall\; x \in S$.
\end{defn}

\begin{thm}[Extreme value theorem] \label{thm:extreme value}
    Let $f : [a, b] \to \R$ be a continuous function. Then $f$ attains both a global maximum and a global minimum in $[a, b]$.
\end{thm}
\begin{proof}
    Since $f$ is bounded on $[a, b]$, \[
        s = \sup f([a, b])
    \] exists.
    Suppose $f(x) < s \;\forall\; x \in [a, b]$. Let \[
        g(x) := s - f(x)
    \] on $[a, b]$. Note that $g$ is continuous and positive. Thus (by HW6.2), there exists a $c > 0$ such that $g(x) \geq c \;\forall\; x \in [a, b] \implies f(x) \leq s - c \;\forall\; x \in [a, b]$. A contradiction.
\end{proof}
\begin{cor} \label{cor:continuous fn range}
    Let $f : [a, b] \to \R$ be continuous. Then (using IVT), \[
        f([a, b]) = [\min_{[a, b]} f, \max_{[a, b]} f]
    \]
\end{cor}

% lecture 18

\section{Differentiation}

\begin{defn} \label{defn:dv}
    Let $f : (a, b) \to \R$ be a function and $p \in (a, b)$. We say that $f$ is differentiable in $(a, b)$ if \[
        \lim_{h \to 0} \frac{f(p + h) - f(p)}{h} = \lim_{x \to p} \frac{f(x) - f(p)}{x - p}
    \] exists, and the limit is called the derivative of $f$ at $p$, denoted $f'(p)$.

    If $f$ is differentiable on each $p$ in $(a, b)$, it is said to be differentiable on $(a, b)$ and $f' : (a, b) \to \R$ is called the derivative of $f$ on $(a, b)$.
\end{defn}

\begin{thm}[Differentiability $\implies$ continuity] \label{thm:diff=>cont}
    Let $f : (a, b) \to \R$ be differentiable at $p \in (a, b)$. Then $f$ is continuous at $p$.
\end{thm}
\begin{proof}
    \begin{align*}
        f(x) &= (x - p) \cdot \frac{f(x) - f(p)}{x - p} + f(p) \\
        \lim_{x \to p} f(x) &= 0 \cdot f'(p) + f(p) \tag{exists} \\
        &= f(p) \qedhere
    \end{align*}
\end{proof}

\begin{thm}[Algebra of derivatives] \label{thm:dv algebra}
    Let $f, g : (a, b) \to \R$ be differentiable at $p \in (a, b)$. Then
    \begin{enumerate}[label=(\alph*)]
        \item $f + g$ is differentiable at $p$ and $(f + g)' = f' + g'$.
        \item $f - g$ is differentiable at $p$ and $(f - g)' = f' - g'$.
        \item $f \cdot g$ is differentiable at $p$ and $(f \cdot g)' = f' \cdot g + f \cdot g'$.
        \item $f / g$ is differentiable at $p$ if $g \neq 0$ and $(f / g)' = \frac{f' \cdot g - f \cdot g'}{g^2}$.
    \end{enumerate}
\end{thm}
\begin{proof}
    Quotient rule: for the special case of $f(x) = 1$, we have
    \begin{align*}
        \lim_{h \to 0} \frac{\frac{1}{g(p + h)} - \frac{1}{g(p)}}{h} &= - \lim_{h \to 0} \frac{g(p + h) - g(p)}{h \cdot g(p + h) \cdot g(p)} \\
        &= - \frac{1}{g(p)^{2}} \lim_{h \to 0} \frac{g(p + h) - g(p)}{h} \\
        &= - \frac{1}{g(p)^{2}} g'(p) \tag{exists}
    \end{align*}
\end{proof}

\begin{example}
    \begin{enumerate}[label=(\alph*)]
        \item (Constant function) $f(x) = c$ is differentiable at $p$ for any $c \in \R$ and $f'(p) = 0$.
        \begin{proof}
        \[
            \lim_{h \to 0} \frac{f(p + h) - f(p)}{h} = \lim_{h \to 0} \frac{c - c}{h} = 0 \qedhere
        \]
        \end{proof}
        \item $f(x) = x^{n}, x \in \R, n \in \Z \setminus \set{0}$ is differentiable at $p$ and $f'(p) = n \cdot p^{n - 1}$.
        \begin{proof}
            \begin{align*}
                \lim_{h \to 0} \frac{f(p + h) - f(p)}{h} &= \lim_{h \to 0} \frac{(p + h)^{n} - p^{n}}{h} \\
                &= \lim_{h \to 0} \frac{p^{n} + n \cdot p^{n - 1} \cdot h + \cdots + n \cdot h^{n - 1} \cdot p + h^{n} - p^{n}}{h} \\
                &= \lim_{h \to 0} \frac{n \cdot p^{n - 1} \cdot h + \cdots + n \cdot h^{n - 1} \cdot p}{h} \\
                &= n \cdot p^{n - 1} \qedhere
            \end{align*}
        \end{proof}
        As a consequence, we get that polynomials and rational functions are continuous in their domains.

        \item $f(x) = \sin x$ is differentiable at $p$ and $f'(p) = \cos p$.
        \begin{proof}
            \begin{align*}
                \lim_{h \to 0} \frac{f(p + h) - f(p)}{h} &= \lim_{h \to 0} \frac{\sin(p + h) - \sin p}{h} \\
                &= \lim_{h \to 0} \frac{\sin p \cos h + \cos p \sin h - \sin p}{h} \\
                &= \cos p \lim_{h \to 0} \frac{\sin h}{h} + \sin p \lim_{h \to 0} \frac{\cos h - 1}{h} \\
                &= \cos p + \sin p \lim_{h \to 0} \frac{-2 \sin^{2} \frac{h}{2}}{h} \\
                &= \cos p + \sin p \cdot 0 \\
                &= \cos p \qedhere
            \end{align*}
        \end{proof}

        \item $f(x) = \cos x$ is differentiable at $p$ and $f'(p) = - \sin p$.
        \begin{proof}
            \begin{align*}
                \lim_{h \to 0} \frac{f(p + h) - f(p)}{h} &= \lim_{h \to 0} \frac{\cos(p + h) - \cos p}{h} \\
                &= \lim_{h \to 0} \frac{- \sin p \sin h + \cos p \cos h - \cos p}{h} \\
                &= - \sin p \lim_{h \to 0} \frac{\sin h}{h} + \cos p \lim_{h \to 0} \frac{\cos h - 1}{h} \\
                &= - \sin p + \cos p \lim_{h \to 0} \frac{-2 \sin^{2} \frac{h}{2}}{h} \\
                &= - \sin p + \cos p \cdot 0 \\
                &= - \sin p \qedhere
            \end{align*}
        \end{proof}

        \item $f(x) = \abs{ x }$ is continuous but not differentiable at $p = 0$.
        \begin{proof}
            \begin{align*}
                \frac{\abs{ h }}{h} &= \begin{cases}
                    1 & h > 0 \\
                    -1 & h < 0 \\
                    \text{undefined} & h = 0
                \end{cases}
            \end{align*}
            Let $a_{n} = \frac{(-1)^{n}}{n}$, then $\lim_{n \to \infty} a_{n} = 0$ but $\lim_{n \to \infty} \frac{\abs{ a_{n} }}{a_{n}} = \lim_{n \to \infty} (-1)^{n}$ does not exist.
        \end{proof}
    \end{enumerate}
\end{example}

\begin{defn}[Inverse function] \label{defn:inverse}
    Let $f : A \to B$ be bijective. Then for any $y \in B$, there exists (unique) $x_{y} \in A$ such that $f(x_{y}) = y$. We define the inverse function $f^{-1} : B \to A$ as \[
        f^{-1}(y) = x_{y}.
    \] and say that $f$ is invertible on $A$.

    Note that $(f \circ f^{-1})$ and $(f^{-1} \circ f)$ are the identity functions on $B$ and $A$ respectively.
    
    For example, the function $f(x) = x^{2}$ is invertible on $\R^{+}$ and its inverse is $f^{-1}(x) = \sqrt{x}$.
\end{defn}

\begin{thm}[Monotone/Continuous/Differentiable Inverse Function Theorem] \label{thm:inverse fn}
    Let $f : [a, b] \to \R$ be an invertible function on $[a, b]$ with range $J$.
    \begin{enumerate}[label=(\roman*)]
        \item If $f$ is (strictly) increasing, then so is $f^{-1}$.
        \item If $f$ is continuous, then $f : [a, b] \to J$ is strictly monotone and $f^{-1} : J \to [a, b]$ is continuous.
        \item If $f$ is differentiable at $p \in (a, b)$ and $f'(p) \neq 0$, then $f^{-1}$ is differentiable at $f(p) = q \in J$ and $(f^{-1})'(q) = \frac{1}{f'(p)}$.
    \end{enumerate}
\end{thm}
\begin{proof}
    \begin{enumerate}[label=(\roman*)]
        \item If $x_{1}, x_{2} \in [a, b]$, then $f(x_{1}) < f(x_{2})$ implies $x_{1} < x_{2}$, and hence $f^{-1}(f(x_{1})) < f^{-1}(f(x_{2}))$.
        \item Given: $f : [a, b] \to J$ is invertible and continuous. Thus, $J = [A, B]$.
        \begin{enumerate}[label= (Case \arabic*)]
            \item ($f(a) < f(b)$) We prove that $f$ is strictly increasing. Define \[
                g(x) = f(x) - f(a).
            \] Then $g : [a, b] \to \R$ is continuous and $g(a) = 0$. Since $f$ is continuous, $g$ is also continuous. Since $f(a) < f(b)$, $g(b) > 0$. For $c \in (a, b]$, $g(c)$ is non-zero as $f$ is injective. If $g(c) < 0$, then by IVT it is 0 somewhere in $(c, b)$, a contradiction. Thus $g(x) > 0 \iff f(x) > f(a) \;\forall\; x \in (a, b]$.

            Similarly for any $x_{0} \in (a, b)$, $g(x) > 0 \iff f(x) > f(x_{0}) \;\forall\; x \in (x_{0}, b]$. Hence $f$ is strictly increasing.

            \item ($f(a) > f(b)$) We prove that $f$ is strictly decreasing. Define \[
                g(x) = f(a) - f(x).
            \] Then $g : [a, b] \to \R$ is continuous and $g(b) = 0$. Since $f$ is continuous, $g$ is also continuous. Since $f(a) > f(b)$, $g(b) > 0$. For $c \in (a, b]$, $g(c)$ is non-zero as $f$ is injective. If $g(c) < 0$, then by IVT it is 0 somewhere in $(a, c)$, a contradiction. Thus $g(x) > 0 \iff f(a) > f(x) \;\forall\; x \in (a, b]$.

            Similarly for any $x_{0} \in (a, b)$, $g(x) > 0 \iff f(x_{0}) > f(x) \;\forall\; x \in (a, x_{0})$. Hence $f$ is strictly decreasing.
        \end{enumerate}
        Let $p \in [a, b]$. We show that $f^{-1}$ is continuous at $f(p)$. Suppose WLOG that $f$ is increasing. Let $\varepsilon > 0$. Let $\delta = \min \set{f^{-1}(\max \{f(p) - \varepsilon), f(a)}), f^{-1}(\min \set{f(p) + \varepsilon, f(b)})\}$

        \item Given: $f : [a, b] \to J$ is invertible and differentiable at $p \in (a, b)$ with $f'(p) \neq 0$. Thus, $J = [A, B]$.
    \end{enumerate}
\end{proof}


\end{document}




